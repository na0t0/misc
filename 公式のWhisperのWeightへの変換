from transformers import AutoProcessor, WhisperForConditionalGeneration
finetune_model = WhisperForConditionalGeneration.from_pretrained("whisper-small-ja/checkpoint-40")

WHISPER_MAPPING = {
    "encoder.ln_post.weight": "encoder.layer_norm.weight", # added by ax
    "encoder.ln_post.bias": "encoder.layer_norm.bias", # added by ax
    "blocks": "layers",
    "mlp.0": "fc1",
    "mlp.2": "fc2",
    "mlp_ln": "final_layer_norm",
    ".attn.query": ".self_attn.q_proj",
    ".attn.key": ".self_attn.k_proj",
    ".attn.value": ".self_attn.v_proj",
    ".attn_ln": ".self_attn_layer_norm",
    ".attn.out": ".self_attn.out_proj",
    ".cross_attn.query": ".encoder_attn.q_proj",
    ".cross_attn.key": ".encoder_attn.k_proj",
    ".cross_attn.value": ".encoder_attn.v_proj",
    ".cross_attn_ln": ".encoder_attn_layer_norm",
    ".cross_attn.out": ".encoder_attn.out_proj",
    "decoder.ln.": "decoder.layer_norm.",
    "encoder.ln.": "encoder.layer_norm.",
    "token_embedding": "embed_tokens",
    "encoder.positional_embedding": "encoder.embed_positions.weight",
    "decoder.positional_embedding": "decoder.embed_positions.weight",
    #"ln_post": "layer_norm", # disabled by ax
}

def rename_keys(s_dict):
    keys = list(s_dict.keys())
    for key in keys:
        new_key = key
        for v, k in WHISPER_MAPPING.items():
            if k in key:
                new_key = new_key.replace(k, v)

        print(f"{key} -> {new_key}")

        s_dict[new_key] = s_dict.pop(key)
    return s_dict

state_dict = finetune_model.model.state_dict()
rename_keys(state_dict)

import whisper
model = whisper.load_model("small")

missing, unexpected = model.load_state_dict(state_dict, strict = False)

if len(missing):
    print("Weight name not found", missing)
    raise

result = model.transcribe("test/axell_130.wav", language="ja", verbose=False)

for s in result["segments"]:
    start = s['start']
    end = s['end']
    text = s['text']
    print(str(start) + "\t" + str(end) + "\t" + text)
